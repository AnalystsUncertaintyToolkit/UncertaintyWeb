---
title: <br><br><br> # Mitigating uncertainty
output:
  html_document:
    toc: true
    toc_float:
          collapsed: false
    toc_depth: 2
    includes:
       in_header: tracking.html
---

<style type="text/css">

#TOC {
margin-top: 100px;
}

.list-group-item.active, .list-group-item.active:hover, .list-group-item.active:focus {
background-color: DARKBLUE;
} 

body {
counter-reset : h1;
}

h1 {
  counter-reset:h2;
}

h2 {
counter-reset : h3;
}

h1:before{
  content : "3" ". ";
  counter-increment: h1;
}

h2:before {
content : "3" "." counter(h2,decimal) ". ";
counter-increment : h2;
}

h2.nocount:before {
content : "";
counter-increment : none;
}

.section h2{
padding-top: 200px;
float:right;
width: 75%;
}

hr { 
  display: block;
  margin-top: 0.5em;
  margin-bottom: 0.5em;
  margin-left: auto;
  margin-right: auto;
  border-style: inset;
  border-width: 1px;
  width: 75%;
  float: right;
} 

.section h3{
float:right;
width: 75%;
}

.leftA{
float: left;
width: 100%;
font-weight: bold;
}

.rightA{
float:right;
width: 75%;
}

.leftB{
float: left;
width: 23%;
text-align: right;
}

.footer{
  position: relative; 
  margin-top: 100px; 
  height = 100px; 
  width: 100%;
  text-align: left
}


.
</style>

<div class="pageContentWrapper">

# Mitigating uncertainty

<!-- introduction here?*-->

## Mitigating uncertainty in quantitative data analysis

<div class = "rightA">
If the data is missing across all observations, for example due to test design, failure in the observations or failure in recording observations, the data can be classified as missing completely at random (MCAR). This is because the reasons for its absence are external and not related to the value of the observation. It is typically safe to remove MCAR data because the results will be unbiased. The statistical test(s) you are performing may not be as powerful, but the results will be reliable.
<br>

Listwise deletion involves deleting all data for an observation that has one or more missing values. The analysis is run only on observations that have a complete set of data. If the number of observations with missing data is small, it may be the most efficient method to handle missing data. However, if listwise deletion would result in throwing away a lot of your data, pairwise deletion may be more appropriate. Here, cases with missing data are used when analysing variables where they don’t have missing values. For example, if you are missing data on a participant's gender then this participant would be excluded from any analysis using the gender variable but would be included in any analyses for which the participant has complete data. However, the resulting statistics may vary because they are based on different data sets.
<br>

If the data is missing systematically, imputation can be more appropriate. The imputation method develops reasonable guesses for missing data. A common technique used when the number of missing cases is small is to impute the mean or median. For datasets with a large amount of missing data, multiple imputation is often used. Here, instead of substituting a single value for each missing data point, multiple imputed data sets are created where the missing values are exchanged for values that encompass the natural variability and uncertainty of the observed values. Each set is then analysed using the standard analytical procedures, and the multiple analysis results are combined to produce an overall result.

The website <a href="https://www.mastersindatascience.org/learning/how-to-deal-with-missing-data/#:~:text=%2520How%2520to%2520Deal%2520with%2520Missing%2520Data%2520,a%2520large%2520amount%2520of%2520missing%2520data.%2520More%2520" style="color: DARKBLUE;"><b>How to Deal with Missing Data</b></a> provides a useful overview of methods to deal with missing data.
</div>

<div class = "leftB"> 
To decide what method is appropriate to use to deal with missing data, you need to understand why the data is missing.
</div>

<div class = "rightA">
The probability of making a type I error (or a false positive) is represented by your significance level (or alpha level): a 0.05 significance level indicates that you are willing to accept a 5% probability that your results occurred by chance.
<br>

If you need to be very confident that your results are not going to be a false positive, you can decrease your significance level, for example to 0.01. Because this change increases the amount of evidence required to conclude a difference is significant, it makes your test less sensitive to detecting differences, but decreases the chance of committing a Type I error (or a false positive) occurring from 5% to 1%.
<br>

However, using a lower value for alpha means that you will be less likely to detect a true difference if one really exists, which increases the risk of a type II error occurring.
</div>

<div class = "leftB"> 
If you need to be very confident that your results are not going to be a false positive (concluding there is a significant difference when it has actually occurred by chance), you can decrease your significance level
</div>

<div class = "rightA"> 
A type II error is when you conclude there is no significant difference when one does in fact exist. You can reduce the risk of a type II error by ensuring your sample size is large enough to detect a difference when one truly exists.
</div>

<div class = "leftB">
You can decrease your risk of committing a type II error by ensuring your test has enough statistical power
</div>

<div style="clear: both;"></div>

<div class = "rightA">
Synthesis methods include systematic review and meta-analyses. These methods include reviewing study characteristics and quality and, where relevant, combine data in a statistical synthesis of study findings. Evidence synthesis (i.e. systematic reviews and meta-analyses) sit at the top of the ‘evidence pyramid’. This means they are considered the evidence source with the least uncertainty and highest rigour as their design minimises bias and maximises your ability to ascertain causality. However, the types of evidence at the top of the pyramid may not be available or feasible for your research topic of interest, in which case you will need to move down the evidence pyramid (note that this is not a comprehensive list of potential evidence sources):
</div>

<div class = "leftB">
Combining findings from a range of studies, known as synthesis, allows you to draw conclusions from a body of evidence
</div>

```{r fig.align="center", echo=FALSE, alt = ""}

knitr::include_graphics("images/evidence_pyramid.png")

```

<div style="clear: both;"></div>



## Mitigating uncertainty in experimental and quasi-experimental evaluation methods

<div style="clear: both;"></div>

<hr>

<div style="clear: both;"></div>

<div class = "rightA"> 
This depends on the collection of comparable data from both the intervention and control groups to ensure you can measure the intervention effect. In addition, the intervention effect must be large enough to be distinguished from noise in the data. Details of how to design a counterfactual for quasi-experimental and experimental studies can be found in the <a href="https://www.gov.uk/government/publications/the-magenta-book" style="color: DARKBLUE;"><b>Magenta Book</b></a>.
</div>

<div class = "leftB">
The counterfactual and intervention group must be comparable
</div>

<div class = "rightA">
Details of which methods are appropriate for different situations are explained in section 3.5 of the <a href="https://www.gov.uk/government/publications/the-magenta-book" style="color: DARKBLUE;"><b>Magenta Book</b></a>.
<br>

Where no comparison group is available, as in the case of analysing the impact of a global event like the COVID-19 pandemic or 2008 economic crisis, you can use an interrupted time-series approach. However, this is only appropriate where changes are sudden. This approach option is preferred to binary before-after comparisons which can be misleading as they present data out of the context of underlying trends.
</div>

<div class = "leftB">
Where it is not possible to run an experimental study to measure impact, you should select a quasi-experimental method based on the availability of a comparison group
</div>

<div class = "rightA">
Developing a comprehensive theory of change to explain how inputs of the intervention lead to outputs, outcomes and impact and testing it against existing evidence will help identify what data is needed for an impact evaluation. Having a robust, high quality theory of change will also ensure that you do not miss any key indicators and alert you to any potential unintended consequences that may need to be measured.
<br>

There is a useful logic models guide on gov.uk. This goes through the process of creating a logic model and provides examples and templates. There is also some good guidance on developing and using logic models from the <a href="https://www.nuffieldtrust.org.uk/files/2019-02/stephanie-kumpunen-and-muna-sheikh.pdf" style="color: DARKBLUE;"><b>Nuffield Trust</b></a> and a step by step guide from the US CDC.
</div>

<div class = "leftB">
Ensuring you have a solid grounding in the theory underpinning your intervention is essential for an effective evaluation design.
</div>

<div class = "rightA">
Comparing estimated effect sizes to effect sizes from other studies in different contexts can be useful for setting realistic expectations of effects and thereby guide the research design (e.g. required sample size, whether effects are homogeneous or heterogeneous). Benchmarking can also help to understand similarities and differences between the effects of interventions in different contexts.
</div>

<div class = "leftB">
A baseline, or starting point, can also serve as a benchmark against which future progress and effects of an intervention can be assessed.
</div>

<div style="clear: both;"></div>

<div class = "rightA">
Theory-based impact evaluations make conclusions about the effect of an intervention through testing the causal pathways through which the change is brought about. This method assesses whether the evidence is sufficient to support these causal pathways and that alternative explanations can be ruled out. The Magenta Book provides further guidance on theory-based evaluation.
</div>

<div class = "leftB">
Where data is limited or the intervention will produce small or unpredictable effects, theory-based evaluation could be an alternative.
</div>

<div style="clear: both;"></div>


## Mitigating uncertainty in survey research

<div style="clear: both;"></div>

<hr>

<div style="clear: both;"></div>

<div class = "rightA">

There are numerous different ways to reduce the risk of uncertainty posed by your data collection methods. These include:
<br>

<b>Questionnaire testing</b>
<br>

Piloting: testing a questionnaire with a small group of experts is a useful method for ensuring questions are appropriately worded and likely to be understood by the target audience. It is a useful test of face validity. That is, it tests whether the question, on the surface, taps into the concepts it’s intended to measure.
<br>

Cognitive testing: this is a form of qualitative research with prospective survey respondents to understand how they interpret and answer specific questions and what they think about when completing the survey. This helps to ensure results of the survey are properly understood by the researcher and that the questions make sense to the respondent and elicit the kind of information expected.
<br>

Question testing: Item non-response occurs when a respondent completes a survey but fails to provide an answer on specific items. The amount of item non-response is considered a useful indicator of data quality. If particular questions are commonly left unanswered, it may indicate a problem (e.g. respondents may not understand what’s being asked, the question may be sensitive, the response categories are insufficient and so the respondent cannot adequately answer, etc.). Item non-response is a particular concern if it occurs systematically (i.e. particular types of respondents are less likely to answer a particular question). When conducting a survey, testing for item non-response helps to identify issues with particular questions and any systematic bias that might introduce.
<br>

<b>Question sequencing and randomisation</b>
<br>

To minimise the priming effects arising from the order of questions, researchers should carefully decide the sequence of questions, as well as considering whether rotating the order of certain questions is appropriate. Furthermore, it’s common practice to randomise the order of response options where these are codes or statements, and invert the order of the scale for half the respondents for questions with a rating scale as the response options.

<b>Using scripting and logic in online surveys</b>
<br>

To increase data accuracy, for online surveys you can use methods such as only showing particular questions to respondents as a result of their responses to previous questions and ensuring that several answer options cannot be chosen on questions where only one answer option should be chosen.
<br>

<b>The size of your sample</b>
<br>

Larger samples will tend to be more representative (assuming you are conducting random sampling). Keep in mind that it’s unlikely that every sample will be perfectly similar to the population of interest. There will always be a little sampling error associated with any study, unless you sample every single member of your population.
<br>

<b>The likely response rates of your survey</b>
<br>

You may need to contact a lot more people than you need to achieve a sample that is representative of your population under study and allows you to draw conclusions within the margin of error you are willing to accept.
<br>

<b>Your sampling frame (your possible participants) and recruitment procedures</b>
<br>

Avoid only recruiting members of a certain subset of your population. Ideally you would randomly sample from your sample frame. Through this, you minimize any selection biases that might occur, such as volunteer bias.
Selection bias occurs when the subjects studied are not representative of the target population about which conclusions are to be drawn. However, selection bias is only problematic if the response probability is correlated with the variable you are measuring, i.e. the people who respond to the survey answer differently to your questions to the way in which those who did not respond would have done. Thus, addressing selection bias requires breaking the connection between non-response probability and outcome variables: adaptive response design.
<br>

<b>Implementing a stratification protocol, such as proportionate stratified sampling</b>
<br>

Let’s say you do your research and find out your population under study are 80% women. You could then make sure that 80% of your sample consists of women, such as by quota sampling.
<br>

Alternatively, you might want to consider over-sampling: this is the selection of a large number of additional respondents that match certain criteria, to allow researchers to measure more precisely the changes in key populations.While this might not result in your overall sample representing the overall population under study (e.g. the general population), this may provide the most useful approach to allow you to draw conclusions from a small sub-group, such as a particular ethnic group.
<br>

<b>Using sample weights to correct for the over-representation or under-representation of key groups</b>
<br>

You can weight down the responses from the over-represented group (which may have been purposefully oversampled) to make sure their views do not have a disproportionately large effect when conclusions are drawn based on responses from the whole sample, or weight up the responses from under-represented group to make sure they don’t have a disproportionately small effect.
<br>

A sample weight is a statistical measurement which is linked to the record of every survey respondent. The sample weight is calculated based on the probability of being selected for the survey for the respondent and can also account for other imbalances which arise in the sampling process, such as non-response adjustment. The value of a weight can be interpreted as a measure of the number of population members represented by that respondent.
<br>

For example, if 51% of a population are female, but a sample is only 40% female, then weighting is used to correct for this imbalance. There are a number of different types of weights:
<br>

<i>Design weights</i>
<br>

We use design weights to account for the different probabilities of being sampled that different respondent types have. Let’s say we’re collecting data based on a list of addresses. People who live in a place where many families share the same address will have a lower chance of being surveyed than people who live at single-family addresses. Weighting our survey results ensures our results won’t be skewed by this discrepancy.
<br>

<i>Non-response weights</i>
<br>

You can use non-response weights to correct for the fact that some types of people are less likely to be willing to participate in your survey than others. To illustrate, let’s imagine that young people in our district are less inclined to answer our survey questions. Weighting our results ensures that we account for this fact by placing more load on the responses from young people who do participate.
<br>

<i>Calibration/post-stratification weights</i>
<br>

You can use calibration weights to make the characteristics of your sample closely match the characteristics of your population. This is commonly done using demographic data (like gender, age, income level) that is publicly available (from a Census, for example) as the target and adjusting the sample demographics to match that target.
<br>

Weights almost always increase the standard errors of estimates and introduce instability into your data. Some researchers like to “trim” the weights to not allow extremely high weights that can increase instability of estimates, but trimming the weights can often result in reducing the representativeness of the weighted data – it’s a trade-off between less instability or more accurate representativeness.

</div>

<div style="clear: both;"></div>

<div class="footer">
<b>Useful links:</b>
<br><br>
<a href="accessibility.html">Click here to see the accessibility statement</a>
<br><br>
<a href="index.html">Click here to return to home page</a>
</div>

